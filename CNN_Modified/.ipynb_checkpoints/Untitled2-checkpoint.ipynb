{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "435e0e37-e2b3-46ff-b5a8-b920492b8136",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "-  This is a type of artificial neural network designed specifically for processing and analyzing visual data, such as images and videos.  In CNN, every image is represented in the form of an array of pixel values. CNNs have proven to be highly effective in various computer vision tasks, including image classification, object detection, image segmentation, and more. CNN's neurons are arranged like the brain's frontal lobe, the area responsible for processing visual stimuli.\n",
    "## Convolution Operation:\n",
    "- This operation involves sliding the filter over the input data and performing element-wise multiplication and summation to compute a response value. The result is stored in a feature map.\n",
    "- Firstly image is taken as an input and and every image is represented in array of pixel values. W e take 2 arrays and multiply first 3 elements and then we add the values of array which are taken as output of multiplication. Then again next 3 elements of the array is taken and same process is done in one dimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a7e269-3088-416a-b070-e17008d05c93",
   "metadata": {},
   "source": [
    "## How does CNN Recognize an Image:\n",
    "\n",
    "- ***Filters:***  Imagine you're looking at a picture, and you want to find specific things in that picture, like edges, colors, or shapes. Filters in CNNs are like special glasses you wear to help you find those things.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31db6a8b-4a98-4dc2-8c5a-b423ddb06a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # importing libraries\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2122486-3156-43aa-8e2e-cd6f300aff5e",
   "metadata": {},
   "source": [
    "-  ***Momentum*** is a parameter used in some optimization algorithms to accelerate the convergence of training. Convergence indicates that the model or algorithm has effectively learned from the data or has found the best possible set of parameters to minimize a particular objective (e.g., loss function) or solve a specific task.\n",
    "-  ***log_interval:*** This variable defines how often (after how many mini-batches) you want to log or display training information. In our code, we are logging information after every 10 mini-batches.\n",
    "-  ***random_seed:*** Setting a random seed ensures that your results are reproducible. By setting the random seed to 1, you can obtain the same randomization results each time you run your code.\n",
    "-  ***Disabling the CuDNN backend*** for PyTorch means that you are instructing PyTorch not to use the CuDNN library for GPU acceleration when performing certain deep learning operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635320ff-ac43-4498-8f58-65a49cff4868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1794b536470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 100 # defining no of epochs. We have set it to 1, meaning we will train the model for only one epoch.\n",
    "batch_size_train = 64 #defining batch size for training. It means that the training data will be divided into mini-batches of 64 samples each.\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01 #  It determines how quickly or slowly the model learns. In our code, we have set it to 0.01.\n",
    "momentum = 0.5 \n",
    "log_interval = 10\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af404d28-32dd-4606-9fb3-6651b4af5a3a",
   "metadata": {},
   "source": [
    "***These data loaders are essential for loading and batching the data during the training and testing phases of a machine learning model.*** \n",
    "\n",
    "## Training Data Loader (train_loader)\n",
    "- It loads the MNIST training dataset.\n",
    "- The dataset is located in the '/files/' directory, which is the specified root directory.\n",
    "- ***train=True*** specifies that you are loading the training split of the dataset.\n",
    "- ***download=True*** will download the dataset from the internet if it's not already present in the specified directory.\n",
    "- ***transform*** specifies a sequence of data transformations to be applied to each image in the dataset.\n",
    "- ***shuffle=True*** indicates that the data will be shuffled before each epoch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80bff038-6f3f-465c-898e-289760216da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb42c3b-55e2-48b5-b761-2bc4ff03c3fd",
   "metadata": {},
   "source": [
    "- ***test loader*** represents like a big box which holds all data.\n",
    "- The small boxes inside the big box are the individual batches of data, each containing a set of images and their labels.\n",
    "- The ***enumerate*** function is like having a numbered sticker on each small box.\n",
    "- ***next(examples)*** is like reaching into the big box, taking out the next small box (with its numbered sticker), and opening it to see what's inside.\n",
    "- We will have extracted one batch of test data (example_data) and their corresponding labels (example_targets) from the MNIST test dataset. We can use this data to make predictions with our trained machine learning model and evaluate its performance on this batch of test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b9c1a5-cb21-4031-9d9a-54fe36f88036",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader) \n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244ae2ed-68cb-4baa-b6c0-7224c1295843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data.shape # It explains batch size, channel, height, width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfb6929-ee0c-4592-b4d3-89195180253d",
   "metadata": {},
   "source": [
    "-  ***plt.subplot(2,3,i+1):*** In subplot 2 and 3 represents number of rows and columns. Matplotlib uses 1-based indexing for subplots\" means that when you specify the index of a subplot within a grid of subplots, you should start counting from 1, not 0.\n",
    "-  ***plt.tight_layout():*** This function is called to ensure that subplots are properly spaced and don't overlap.\n",
    "- ***plt.imshow(example_data[i][0]***, cmap='gray', interpolation='none'): This line displays the current image (indexed by i) from the example_data tensor. Since these are grayscale images, cmap='gray' is used to display them in grayscale.\n",
    "- ***plt.title(\"Ground Truth: {}\".format(example_targets[i])):*** It sets the title for the current subplot, indicating the ground truth label for the displayed image.\n",
    "- ***plt.xticks([]) and plt.yticks([]):*** These lines remove the x and y-axis ticks from the subplot, making the image cleaner without axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14bd4d70-822c-4eb0-88c5-00b4701137bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WALEED TRADERS\\AppData\\Local\\Temp\\ipykernel_16360\\608570063.py:6: UserWarning: The figure layout has changed to tight\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6): # This loop iterates over the first 6 images in your batch. You're going to display these 6 images.\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    # it should display the image exactly as it is, without any smoothing or interpolation between pixel values.\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ee62a2a-0128-493b-b44c-5a60a5f9a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # it contains necessary activation function.\n",
    "import torch.optim as optim # importing optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceee9fa-7b9d-4002-8aec-cc747370aec7",
   "metadata": {},
   "source": [
    "## Creating Class Of Model:\n",
    "- ***self.conv1 = nn.Conv2d(1, 10, kernel_size=5):*** This line defines the first convolutional layer. It takes input images with one channel (grayscale) and applies 10 filters (also known as kernels) with a kernel size of 5x5. This layer is responsible for learning local patterns and features in the input images.\n",
    "- ***self.conv2 = nn.Conv2d(10, 20, kernel_size=5):*** This line defines the second convolutional layer. It takes the output of the first convolutional layer (which has 10 channels) and applies 20 filters with a kernel size of 5x5. This layer further extracts higher-level features from the input.\n",
    "- ***self.conv2_drop = nn.Dropout2d():*** This line defines a 2D dropout layer. Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of the input units to zero during each forward pass.\n",
    "- ***self.fc1 = nn.Linear(320, 50):*** This line defines a fully connected (dense) layer with 320 input features and 50 output features. The 320 input features come from the output of the second convolutional layer (20 channels of 4x4 spatial dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47fb5ba5-08ac-4b24-b0e0-c69ce2529ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): # nn.Module as a base class.\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) \n",
    "# \"10 channels\" refers to the ten output feature maps or channels produced by the first convolutional layer, each capturing different learned features.\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50) # It is used to capture global patterns in the data.\n",
    "        self.fc2 = nn.Linear(50, 10) # It is typical used for a classification task with 10 classes (e.g., digits 0-9).\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
    "#self.conv1(x)applies the first convolutional layer conv1 to the input x. This operation extracts features from the input using the learned convolutional filters.\n",
    "#performs 2x2 max-pooling on the output of the first convolutional layer.\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "# Dropout is used to avoid overfitting. This line is similar to the first one but applied to the output of the second convolutional.\n",
    "        x = x.view(-1, 320) # the data is reshaped using view\n",
    "#-1 is used as a placeholder for the batch size, and 320 represents the number of features produced by the convolutional layers and pooling.\n",
    "        x = F.relu(self.fc1(x)) # This line applies a fully connected (dense) layer fc1 to the flattened x.\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c82d1c-9a0e-487f-ba57-d6c4081bf703",
   "metadata": {},
   "outputs": [],
   "source": [
    "network= Net()\n",
    "#  creates an instance of our neural network model.\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "# This function extracts all the learnable parameters (weights and biases) from network.\n",
    "#  Momentum is a technique used to accelerate training convergence. \n",
    "#It helps the optimizer \"remember\" previous gradients and use them to guide the parameter updates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68395135-6f6d-4f68-b322-841cc7f8c852",
   "metadata": {},
   "source": [
    "## Keeping Track of Training and Testing Losses and the Number of Training Iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4a83481-591b-4dae-8b76-3759e74e005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [] # empty list called train_losses to store the training loss values during the training process.\n",
    "# We append the value of the loss after each training iteration or epoch to this list.\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "# these lists and counters help you visualize the training and validation loss trends over time, \n",
    "# It can be crucial for understanding how well your model is learning and whether it might be overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227853cf-53e2-47d4-87e6-7eafa91ae251",
   "metadata": {},
   "source": [
    "## Training a Neural Network Using the Training Data:\n",
    "- ***for batch_idx, (data, target) in enumerate(train_loader):*** This loop iterates over batches of training data using the train_loader. Each iteration processes a batch of data and its corresponding target labels.\n",
    "- ***loss = F.nll_loss(output, target):*** This calculates the loss between the predicted output and the target labels using the negative log-likelihood loss (NLL loss). The output contains the model's predictions, and target contains the ground truth labels.\n",
    "- ***if batch_idx % log_interval == 0:*** This condition checks whether a specified number of batches have been processed (log_interval) and, if so, prints training progress. This can help you monitor the training process at regular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f384ae19-b46c-4852-9fa8-9fe27b3c043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  network.train() # It ensures that all layers operate in the training mode.\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad() # avoids accumulating gradients from previous batches.\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward() #These gradients are used to update the model's weights during optimization.This helps in back propagation.\n",
    "    optimizer.step() #  It updates the model's weights according to the optimization algorithm\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item()) \n",
    "# This appends the current batch's loss value to the train_losses list, which keeps track of the training loss over time.\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "# This appends the training iteration count to the train_counter list. It helps keep track of the total number of training iterations across all epochs.\n",
    "      torch.save(network.state_dict(),'model.pth') #These lines save the state of the model. This line defines path of the model.\n",
    "      torch.save(optimizer.state_dict(),'optimizer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21baa44b-9eb8-4e39-88a6-47055003f094",
   "metadata": {},
   "source": [
    "## Testing a Neural Network Using the Training Data:\n",
    "- ***max(1, keepdim=True)*** is used to find the maximum value along the second dimension (axis 1) of the output tensor for each sample in the batch. This operation identifies the class with the highest predicted probability for each sample. The keepdim=True argument ensures that the result retains the same number of dimensions as the original tensor, so it remains a 2D tensor.\n",
    "- ***correct += pred.eq(target.data.view_as(pred)).sum():*** This line compares the predicted labels (pred) with the target labels (target) and counts the number of correct predictions. It uses .eq() to perform element-wise equality checks and .sum() to count the correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39fb3c56-5e29-4515-a807-095c85cc512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval() # This line sets the neural network (network) in evaluation mode.\n",
    "  test_loss = 0 # To keep track of the total loss.\n",
    "  correct = 0 # This variable correct will be used to count the number of correctly predicted samples.\n",
    "  with torch.no_grad(): # we don't need gradients for the testing phase, and it can save memory.\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "# Above line is used to compute loss between actual and predicted values using nll function.\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c5ed0d6-01ac-4872-aa72-3bde771ec10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path('/results').mkdir(parents=True, exist_ok=True)\n",
    "# This is often used to ensure that a specific directory exists before writing or saving files to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff68e9-5509-4d62-9f35-19ee6b8de712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WALEED TRADERS\\AppData\\Local\\Temp\\ipykernel_16360\\1411381028.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "C:\\Users\\WALEED TRADERS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3096, Accuracy: 924/10000 (9%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313179\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.300776\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.298872\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.276723\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.241102\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.248727\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.189346\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.146075\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.107432\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.010181\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.842805\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.758985\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.671717\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.689134\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.427019\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.280802\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.311029\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.441940\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.140555\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.151660\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.051933\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.894787\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.189509\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.943906\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.874338\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.843033\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.764724\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.757614\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.818882\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.883733\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.796736\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.911369\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.582851\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.694431\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.616652\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.774380\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.713725\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.639354\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.662006\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.828940\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.857846\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.624476\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.624785\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.677195\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.579102\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.559268\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.825651\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.740211\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.639270\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.648673\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.710413\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.448539\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.594281\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.575701\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.666789\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.570193\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.575802\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.631433\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.807735\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.572707\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.615248\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.537688\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.603138\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.316249\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.338390\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.380996\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.299342\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.411713\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.430178\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.459166\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.616826\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.536267\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.519213\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.382430\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.489391\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.555915\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.422415\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.389958\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.519984\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.434671\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.698667\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.515390\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.676495\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.543999\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.552158\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.640933\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.444989\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.535986\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.541946\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.368643\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.321207\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.312041\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.725066\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.380304\n",
      "\n",
      "Test set: Avg. loss: 0.1871, Accuracy: 9435/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.449070\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.480567\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.495312\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.514498\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.300738\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.166547\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.500744\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.425112\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.379583\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.650049\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.569954\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.316788\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.423266\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.616089\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.492152\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.435812\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.299422\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.516670\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.315918\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.338377\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.471997\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.475868\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.461004\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.365181\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.197315\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.247464\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.366785\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.428426\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.310478\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.335296\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.492469\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.348873\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.368192\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.381412\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.492863\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.617273\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.502646\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.354182\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.402656\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.222965\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.212961\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.391840\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.299902\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.234097\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.178227\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.254259\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.508998\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.299151\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.319894\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.430903\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.678783\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.304389\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.269041\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.409906\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.390180\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.428180\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.450761\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.340755\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.489356\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.440167\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.284873\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.194327\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.411585\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.199885\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.498272\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.144542\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.353886\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.464937\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.265841\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.397215\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.225995\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.484111\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.282676\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.289576\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.184452\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.259817\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.353761\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.439782\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.284221\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.338009\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.408964\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.315961\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.329732\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.270668\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.252364\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.201388\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.495958\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.193525\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.353594\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.226541\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.540880\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.471871\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.268020\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.292702\n",
      "\n",
      "Test set: Avg. loss: 0.1163, Accuracy: 9633/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.200291\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.227911\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.393390\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.261480\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.257290\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.317839\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.264318\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.469376\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.490321\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.394919\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.323109\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.356221\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.231510\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.156528\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.267043\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.236254\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.287531\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.287333\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.425402\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.223523\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.481977\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.324456\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.385123\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.339639\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.318737\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.827643\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.519772\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.420605\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.303589\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.307499\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.337545\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.366738\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.193645\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.178116\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.326203\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.228614\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.456392\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.195066\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.269542\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.160260\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.425587\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.199202\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.199013\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.205847\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.331158\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.449202\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.159591\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.280066\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.257466\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.210571\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.404689\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.328678\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.180246\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.433457\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.254222\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.332605\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.487176\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.193778\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.193869\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.333717\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.444560\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.380054\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.207252\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.236003\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.465520\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.323258\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.364221\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.183084\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.235647\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.303399\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.209640\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.237311\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.334094\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.190611\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.368683\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.348690\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.212442\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.236866\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.487415\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.417246\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.410233\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.224197\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.507426\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.219418\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.179777\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.375016\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.314415\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.175622\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.285970\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.336557\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.499000\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.381611\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.112443\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.290098\n",
      "\n",
      "Test set: Avg. loss: 0.0983, Accuracy: 9688/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.174439\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.180677\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.230463\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.295062\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.208693\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.207405\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.220781\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.325615\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.196384\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.326680\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.185453\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.174436\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.328623\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.133992\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.185854\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.315199\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.567048\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.230738\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.224206\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.371351\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.122843\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.295379\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.440227\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.344851\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.264901\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.078462\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.212960\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.277331\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.256768\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.242411\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.234934\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.180619\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.228730\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.255343\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.263780\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.139223\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.295250\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.133040\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.154961\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.199836\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.186787\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.113590\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.105480\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.302966\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.389331\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.270106\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.341369\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.136419\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.189542\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.140836\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.290414\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.260555\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.436819\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.213510\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.186501\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.297593\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.355050\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.263548\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.091423\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.321009\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.224433\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.313352\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.178960\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.182898\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.304260\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.184035\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.196467\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.364766\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.375873\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.177596\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.174340\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.108176\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.228547\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.337428\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.446782\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.449667\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.224646\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.177348\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.283302\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.207514\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.326319\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.301595\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.167066\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.296262\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.262816\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.420351\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.148697\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.323683\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.137893\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.448054\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.346116\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.342257\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.136490\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.339092\n",
      "\n",
      "Test set: Avg. loss: 0.0834, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.255931\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.294298\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.230596\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.287993\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.249300\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.160558\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.128229\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.342863\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.253713\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.107850\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.199817\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.131212\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.230442\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.200978\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.320400\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.319125\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.236868\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.259169\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.284615\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.132425\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.234676\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.427535\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.168130\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.245782\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.370114\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.413174\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.255388\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.308762\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()\n",
    "# First, it evaluates the initial performance of the model on the test dataset.\n",
    "# Then, it enters a loop over a specified number of training epochs.\n",
    "# Inside the loop, it trains the model for one epoch and evaluates it on the test dataset after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3b927-530c-492b-85e0-86b063a32342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f77ded-1ce8-4e02-a49a-6bacd05daf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0908b936-d036-4d0d-a39d-6e9a7d1e66cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  output = network(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e1759a-a3a6-469f-a52e-54bb9c171770",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560837a-fdb7-4d8e-b45e-878f6f7d71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "continued_network = Net()\n",
    "continued_optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                                momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a21c3-2184-4516-a48e-a525a22307bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "network_state_dict = torch.load('model.pth')\n",
    "continued_network.load_state_dict(network_state_dict)\n",
    "\n",
    "optimizer_state_dict = torch.load('optimizer.pth')\n",
    "continued_optimizer.load_state_dict(optimizer_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec3a284-e90e-4e2f-ae20-0d2e5f14b49d",
   "metadata": {},
   "source": [
    "## Creating Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec5c08-9117-47b7-aa7b-ee552c0f7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86f1c8-2643-4f01-bbeb-a5fd7f843e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []  # Initialize test_losses list\n",
    "test_loss = 0  # Initialize test_loss here\n",
    "correct = 0\n",
    "all_preds = []\n",
    "all_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f09d3-fbea-4c84-9e5d-4264a1a353ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    global test_loss, correct, all_preds, all_labels  # Use global to access variables from the global scope\n",
    "    test_loss = 0  # Reset test_loss\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5bfed-bb07-49f3-849a-8a2677a5cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "         output = network(data)\n",
    "         test_loss = F.nll_loss(output, target, reduction='sum').item()  # Use 'reduction' instead of 'size_average'\n",
    "         pred = output.data.max(1, keepdim=True)[1]\n",
    "         correct = pred.eq(target.data.view_as(pred)).sum()\n",
    "            \n",
    "         all_preds.extend(pred.cpu().numpy())\n",
    "         all_labels.extend(target.cpu().numpy())\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_losses.append(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1164434-b1a8-4f54-b0cb-f2f4abdc7008",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3224a7e-584f-4e74-8609-a1b42e45a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix and classification report\n",
    "confusion = confusion_matrix(all_labels, all_preds)\n",
    "classification_rep = classification_report(all_labels, all_preds, target_names=[str(i) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136bebf-d892-4611-9101-3331d46f0156",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899419cf-f106-4414-93f1-78c1873c9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix (you should have this variable from your code)\n",
    "confusion = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=[str(i) for i in range(10)], \n",
    "            yticklabels=[str(i) for i in range(10)])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e8f7b-2b79-40e2-bf1a-a6484af30743",
   "metadata": {},
   "source": [
    "## Classification Report:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
